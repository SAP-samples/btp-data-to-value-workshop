{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83817430",
   "metadata": {},
   "source": [
    "# Book Genre Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf448bc",
   "metadata": {},
   "source": [
    "In this notebook, we will walk through some **text clustering** with open source Python libraries. Let's think about the following scenario: Mr. Cricket, the owner of the best children bookshop in Walldorf, would like to put some order in his book inventory. He would like to classify the books into different categories based on topic similarity. This would allow him to improve customer experience both in his physical store and his web store, by grouping similar items into homogeneous shelves. Pretty nice idea, but how to do that?! 🤔\n",
    "\n",
    "Mr. John asks for help to his SAP trusted partner. Their cosultants, after taking a look at the bookshop book inventory, come up with a plan. Each book in the inventory comes with a very concise description field. They are going to implement some text analysis on this field and group books with a similar content using an unsupervised clustering strategy. Their project will be then based on the following steps:\n",
    "\n",
    "* **1- Text Preprocessing**\n",
    "* **2- Word Embedding**\n",
    "* **3- Text Clustering**\n",
    "\n",
    "![image](https://github.com/SAP-samples/btp-data-to-value-workshop/raw/main/resources/text-clustering.png)\n",
    "\n",
    "Let's put this into practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042accf7",
   "metadata": {},
   "source": [
    "First, we will make sure the required libraries are installed. We will use a set of very common python libraries, for dataframe handling and visualization (pandas, numpy, matplotlib, seaborn), regex and nltk for text cleaning and preprocessing, gensim for the word embedding and sklearn for the clustering. hana_ml will be used in this notebook only to access the book inventory data, that are stored into Mr. Cricket HANA Cloud database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic Python\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "# text preprocessing\n",
    "!pip install regex\n",
    "!pip install nltk\n",
    "\n",
    "# word embedding\n",
    "!pip install gensim\n",
    "\n",
    "# clustering\n",
    "!pip install sklearn\n",
    "\n",
    "# connection to data source in Hana Cloud\n",
    "!pip install hana_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604e990",
   "metadata": {},
   "source": [
    "Then, we use the import statement to import the packages and define aliases, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cec844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1608e",
   "metadata": {},
   "source": [
    "Then, we want to load our book dataset. We use *hana_ml* to create a connection to the HANA Cloud database, and load the book inventory table in a hana dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hana_ml.dataframe as dataframe\n",
    "from notebook_hana_connector.notebook_hana_connector import NotebookConnectionContext\n",
    "conn = NotebookConnectionContext(connectionId = 'HANACLOUD_D2V' )\n",
    "df_hana = (conn.table('SAP_CAPIRE_BOOKSHOP_BOOKS', schema='DATA2VALUE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c788b7",
   "metadata": {},
   "source": [
    "With the **collect** command, we can copy the data from hana cloud to the Jupyter client, in the form of a pandas dataframe. We are now ready to massage our data using all sorts of python tricks!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "books= df_hana.collect()\n",
    "del df_hana\n",
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2b461",
   "metadata": {},
   "source": [
    "We will be focusing our analysis on the description field in particular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "books[['ID','TITLE','DESCR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b70a0",
   "metadata": {},
   "source": [
    "## 1 - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d9f40",
   "metadata": {},
   "source": [
    "In order to use textual data for predictive modeling, the text must be parsed and transformed to a list of words (called “tokenization”). In this process, special characters and punctuation have to be removed. We should also ged rid of the so called 'stop words', that is to say commonly used words without any specific connotation (such as “the”, “a”, “an”, “in”) etc. These are not predictive and you would not want them to be considered in the predictive model. We will use Natural Language Toolkit (NLTK) and Regular Expressions (RegEx) to clean up and **tokenize** our text.\n",
    "\n",
    "If you fancy digging deeper into these techniques, here are a few refecences:\n",
    "* https://tutorialspoint.dev/language/python/removing-stop-words-nltk-python \n",
    "* https://en.wikipedia.org/wiki/Regular_expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8978c1f",
   "metadata": {},
   "source": [
    "Let's first drop missing values in the book description books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books=books.dropna(axis=0,subset=['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304127d7",
   "metadata": {},
   "source": [
    "Prepare the book description as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d930fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords=set(stopwords.words(\"english\")) \n",
    "import regex as re\n",
    "\n",
    "#Transform to lower case\n",
    "books['tokens']=books['DESCR'].apply(lambda x: x.lower())\n",
    "\n",
    "#Remove punctuation\n",
    "books['tokens']=books['tokens'].map(lambda x: re.sub(\"[-,\\.!?;\\'\\(\\)]\", ' ', x))\n",
    "\n",
    "#Remove stopwords\n",
    "books['tokens']=books['tokens'].apply(lambda x: ' '.join([ t for t in x.split() if not t in stopwords]))                                                     \n",
    "\n",
    "# Remove short tokens\n",
    "books['tokens']=books['tokens'].apply(lambda x:' '.join( [t for t in x.split() if len(t) > 1] )) \n",
    "\n",
    "#Remove extra spaces\n",
    "books['tokens']=books['tokens'].map(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "# Remove duplicate tokens\n",
    "books['tokens']=books['tokens'].apply(lambda x: ' '.join(list(dict.fromkeys(x.split()))))                                                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96849ed8",
   "metadata": {},
   "source": [
    "Drop duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books=books.drop_duplicates('tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books[['ID','TITLE','DESCR','tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae1f98",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f07c2e",
   "metadata": {},
   "source": [
    "Global Vectors for Word Representation (**GloVe**), is a **word2vec** model, that is to say an unsupervised learning algorithm for obtaining vector representations for words.  It allows you to take a corpus of text, and transform each word in that corpus into a position in a high-dimensional space.   \n",
    "\n",
    "We could choose to train our word2vec model on our own corpus of book description, but to semplify the process while taking advantage of the precious machine learning open community, we will download a pretrained model that was developed usin a muuuuch broader corpus: Wikipedia. \n",
    "\n",
    "The gensim python library allows us to to that in 2 lines of code. When you drun them, it will take a minute or two.  Each line of the text file contains a word, followed by N numbers. The N numbers describe the vector of the word’s position.  N may vary depending on which model you choosed to donwload. For us, N is 100, since we are using glove.6B.100d.  \n",
    "\n",
    "To learn more about how to use GloVe see:\n",
    "* https://faculty.ai/tech-blog/glove/\n",
    "* https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gensim_api\n",
    "model = gensim_api.load(\"glove-wiki-gigaword-100\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9087c3",
   "metadata": {},
   "source": [
    "Now that we have downloaded tour word2vec model, we can apply it to every book description, to **embed** our book in a multidimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[]\n",
    "for i,book in books.iterrows():\n",
    "    tokens_features=[]\n",
    "    for word in book['tokens'].split():\n",
    "        try:\n",
    "            tokens_features.append(model[word])\n",
    "        except:\n",
    "            continue\n",
    "    features.append(np.mean(np.array(tokens_features),axis=0))\n",
    "    \n",
    "for i in range(100):\n",
    "    feature='f_'+str(i)\n",
    "    books[feature]=[f[i] for f in features]\n",
    "    \n",
    "del features\n",
    "embedding=['f_'+str(i) for i in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a3c63",
   "metadata": {},
   "source": [
    "You can see the results of our text embedding: we associated each book to a 100-dim numerical vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6598087",
   "metadata": {},
   "outputs": [],
   "source": [
    "books[['TITLE']+embedding]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d9c2c",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22346773",
   "metadata": {},
   "source": [
    "Now that each book is represented by a point in a multidimentional space, we can use the *distance* between these points to find out which books are similar to each other. More specifically, we will be performing a cluster analysis: our book sample will be divided into a certain number of groups such that books in the same groups are more similar (close) to books in the same group than those in other groups.  \n",
    "\n",
    "In this exercise, the number of groups or clusters is set to 10.  We will use sklearn KMeans clustering algorith, in the form of MiniBatchKmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "n_clus=10\n",
    "km = MiniBatchKMeans(n_clusters = n_clus, batch_size=50, random_state=42, max_iter=1000)\n",
    "y_kmeans = km.fit_predict(books[embedding])\n",
    "books['kmeans_cluster']=y_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7d7a9",
   "metadata": {},
   "source": [
    "### Cluster Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e4377",
   "metadata": {},
   "source": [
    "To understand if the clustering worked effectively, let's examing the most representative books for each cluster, the books closest to the cluster centroid  If you run the following cell and scroll down to see the result, you will notice a certain homogeneity between descriptions belonging to the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(n_clus):\n",
    "    print('************* ')\n",
    "    print('- CLUSTER ',str(cluster))\n",
    "    print('*************')\n",
    "    most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(books[embedding] - km.cluster_centers_[cluster], axis=1)\n",
    ")\n",
    "    centroid_index= most_representative_docs[0]\n",
    "    centroid=[]\n",
    "    for i in range(100):\n",
    "        feature='f_'+str(i)\n",
    "    for d in most_representative_docs[:10]:\n",
    "        print(books.reset_index().DESCR[d])\n",
    "        print(\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5e139",
   "metadata": {},
   "source": [
    "### Cluster Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b037b",
   "metadata": {},
   "source": [
    "Most datasets have a large number of variables or dimensions along which the data is distributed. Visually exploring the data is challenging. In our case, for instance, our book embedding has **100 dimensions**. To visualize high-dimensional datasets you can use techniques known as **dimensionality reduction**. \n",
    "\n",
    "**t-Distributed Stochastic Neighbor Embedding (t-SNE)*** is a technique for dimensionality reduction that allows to map an high-dimensional distribution to a 2-dim plane. Since this is computationally quite heavy, another dimensionality reduction technique is used in conjunction with it, e.g. **Principal Component Analysis** or **PCA**. PCA is a technique for reducing the number of dimensions in a dataset while retaining most information. It analyzes the correlation between dimensions and attempts to provide a minimum number of variables that keeps the maximum amount of variation or information about the original data distribution. \n",
    "\n",
    "In our example, we will first reduce our dimensions from 100 to 50 using PCA, and eventually using t-sne to visualize our clusters in 2 dimensions. Here is the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4796b790",
   "metadata": {},
   "source": [
    "#### Reduce variables with PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25349055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pca_50 = PCA(n_components=50)\n",
    "pca_result_50 = pca_50.fit_transform(books[embedding])\n",
    "print('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57be982",
   "metadata": {},
   "source": [
    "#### Execute tsne model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17183214",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=0, perplexity=30, n_iter=2000)\n",
    "tsne_pca_results = tsne.fit_transform(pca_result_50)\n",
    "books[\"tsne-1\"] = tsne_pca_results[:,0]\n",
    "books[\"tsne-2\"] = tsne_pca_results[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40651948",
   "metadata": {},
   "source": [
    "#### Plot clusters in a 2-dim plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bb3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x=\"tsne-1\", y=\"tsne-2\", hue=\"kmeans_cluster\", s=30, palette=\"Paired\",\n",
    "                data=books).set(title=\"Glove 100 projection\") \n",
    "\n",
    "sample_titles=['Shadow','Seventeen coffins','Stronger than magic']\n",
    "for t in sample_titles:\n",
    "    x=books.loc[books['TITLE']==t,'tsne-1'].tolist()[0]\n",
    "    y=books.loc[books['TITLE']==t,'tsne-2'].tolist()[0]\n",
    "    plt.annotate(t,(x,y),xytext=(x-70,y),arrowprops={'arrowstyle':'fancy'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23000a",
   "metadata": {},
   "source": [
    "As you can see, we were able to display the clusters in a 2-dim plane. Samples of books dealing with different themes were assigned to different clusters and lay far from each other in the plot. Now that we are satisfied with our clustering analysis, we are only left with the task of saving the clustering model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(km, open(\"description.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610acd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdAt": "2021-09-06T15:37:23Z",
  "createdBy": "stuart",
  "description": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "modifiedAt": "2021-09-06T15:37:23Z",
  "modifiedBy": "stuart",
  "name": "BookGenreClustering.ipynb",
  "scenarioId": "cc9ee05f-9b66-44c4-a649-0a4f9d544650"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
